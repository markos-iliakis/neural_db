{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "neuraldb_2.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "c0362013f7a1e1ec23d2aac9466df98632f52981019dc53f6f17da377e94bf14"
    },
    "kernelspec": {
      "display_name": "Python 3.9.4 64-bit ('pytorch': conda)",
      "language": "python",
      "name": "python394jvsc74a57bd0c0362013f7a1e1ec23d2aac9466df98632f52981019dc53f6f17da377e94bf14"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.4"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "11kjbjg6pJir"
      },
      "source": [
        "import jsonlines\n",
        "import json\n",
        "import torch\n",
        "from transformers import AutoModel, AutoTokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Ao__VWd3DaP"
      },
      "source": [
        "# !pip install jsonlines\n",
        "# !pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QVCa_yBlpJis"
      },
      "source": [
        "with open('/content/train.jsonl', 'r') as json_file:\n",
        "    json_list = list(json_file)\n",
        "\n",
        "train_dbs, test_dbs, dev_dbs = [], [], []\n",
        "\n",
        "for json_str in json_list:\n",
        "    result = json.loads(json_str)\n",
        "    train_dbs.append(result)\n",
        "    # print(f\"result: {result}\")\n",
        "    # print(isinstance(result, dict))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3tyVPLp3pJiy"
      },
      "source": [
        "MODEL_NAME = \"t5-base\"\n",
        "\n",
        "# We need to create the model and tokenizer\n",
        "model = AutoModel.from_pretrained(MODEL_NAME)\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sWf7UxbVHnRC",
        "outputId": "1722dc14-6559-4c78-f7e6-b34c83b047f0"
      },
      "source": [
        "# !pip install datasets"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement datasets.Dataset (from versions: none)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for datasets.Dataset\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SYNjRSCipJiy"
      },
      "source": [
        "\n",
        "\n",
        "from datasets import Dataset, DatasetDict\n",
        "import pandas as pd\n",
        "\n",
        "# perfectIR\n",
        "# train_dataset = []\n",
        "# for db in train_dbs:\n",
        "#   for query in db['queries']:\n",
        "#     query_text = query['query']\n",
        "#     query_facts = query['facts']\n",
        "\n",
        "#     query_facts = [db['metadata']['raw'][fact[0]]['parse_targets'] for fact in query_facts]\n",
        "#     query_facts = [fact for sub_fact in query_facts for fact in sub_fact ]\n",
        "\n",
        "#     text = ''\n",
        "\n",
        "#     for fact in query_facts:\n",
        "#       text += fact + ' '\n",
        "\n",
        "#     text = text[:-1]\n",
        "#     train_dataset.append([query_text, text])\n",
        "\n",
        "# wholeDB\n",
        "train_dataset = []\n",
        "for db in train_dbs:\n",
        "\n",
        "  db_facts = [db['metadata']['raw'][:]['parse_targets']]\n",
        "  db_facts = [fact for sub_fact in db_facts for fact in sub_fact ]\n",
        "\n",
        "  query_facts = ''\n",
        "\n",
        "  for fact in db_facts:\n",
        "    query_facts += fact + ' '\n",
        "\n",
        "  query_facts = query_facts[:-1]\n",
        "\n",
        "  for query in db['queries']:\n",
        "    query_text = query['query']\n",
        "    query_facts = query['facts']\n",
        "\n",
        "    train_dataset.append([query_text, query_facts])\n",
        "\n",
        "df = pd.DataFrame(train_dataset, columns = ['query', 'facts'])\n",
        "td = Dataset.from_pandas(df)\n",
        "dataset_dict = {\"train\": td,\n",
        "           \"test\": td,\n",
        "           \"unsupervised\": td}\n",
        "\n",
        "d = DatasetDict(dataset_dict)\n",
        "d"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1Ssb7gtpJiz"
      },
      "source": [
        "# encoding = tokenizer.encode_plus(train_dataset[0], add_special_tokens = True,    truncation = True, padding = \"max_length\", return_attention_mask = True, return_tensors = \"pt\")\n",
        "def tokenize_function(examples):\n",
        "  print(examples)\n",
        "  return tokenizer(examples[\"query\"],examples[\"facts\"], padding=\"max_length\", truncation=True)\n",
        "tokenized_datasets = d.map(tokenize_function, batched=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRyt0AYVYE5y"
      },
      "source": [
        "tokenized_datasets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6WL3BOr4c8u"
      },
      "source": [
        "# tokenized_datasets.get(\"train\")[\"input_ids\"][0]\n",
        "for x in tokenized_datasets.get(\"train\")[\"input_ids\"][0]:\n",
        "    print(tokenizer.decode(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iGkldhu5pJiz"
      },
      "source": [
        "# from transformers import pipeline\n",
        "# qna = pipeline(\"question-answering\", model=\"t5-base\", tokenizer=\"t5-base\", framework=\"tf\")\n",
        "\n",
        "# from transformers import default_data_collator\n",
        "\n",
        "# data_collator = default_data_collator\n",
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "args = TrainingArguments(\n",
        "    evaluation_strategy = \"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=10,\n",
        "    per_device_eval_batch_size=10,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    output_dir=\"/content/\"\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=tokenized_datasets.get(\"train\"),\n",
        "    eval_dataset=tokenized_datasets.get(\"test\")\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}